#!/usr/bin/env python
from dataclasses import dataclass, asdict
from fnmatch import fnmatch
import fnmatch
import hashlib
from io import BytesIO, TextIOWrapper
import json
import logging
import os.path
import sys
import time
import urllib.request
import urllib.parse
from typing import Iterable, Tuple
from zipfile import ZipFile

logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class ConfigSource:
    url: str
    entries: list[str]
    renames: dict[str, str]

    @staticmethod
    def from_dict(d: dict) -> "ConfigSource":
        return ConfigSource(
            url=d["url"],
            entries=d["entries"],
            renames=d["renames"],
        )


@dataclass(frozen=True)
class Config:
    id: str
    base_url: str
    output_path: str
    sources: dict[str, ConfigSource]

    @staticmethod
    def from_dict(d: dict) -> "Config":
        return Config(
            id=d["id"],
            base_url=d["base_url"],
            output_path=d["output_path"],
            sources={k: ConfigSource.from_dict(v) for k, v in d["sources"].items()},
        )

    @staticmethod
    def load(path: str) -> "Config":
        with open(path) as stream:
            return Config.from_dict(json.load(stream))


@dataclass(frozen=True)
class DatabaseFile:
    hash: str
    size: int

    @staticmethod
    def from_dict(d: dict) -> "DatabaseFile":
        return DatabaseFile(
            hash=d["hash"],
            size=d["size"]
        )


@dataclass(frozen=True)
class DatabaseFolder:
    pass


@dataclass(frozen=True)
class Database:
    base_files_url: str
    db_id: str
    db_url: str
    files: dict[str, DatabaseFile]
    folders: dict[str, DatabaseFolder]
    timestamp: int

    def write(self, path: str) -> None:
        with ZipFile(path, "w") as zip_file:
            db_file = f"{self.db_id}.json"
            with zip_file.open(db_file, "w") as db_stream:
                json.dump(asdict(self), TextIOWrapper(db_stream))

    @staticmethod
    def from_dict(d: dict) -> "Database":
        return Database(
            base_files_url=d["base_files_url"],
            db_id=d["db_id"],
            db_url=d["db_url"],
            files={k: DatabaseFile.from_dict(v) for k, v in d["files"].items()},
            folders={k: DatabaseFolder() for k in d["folders"].keys()},
            timestamp=d["timestamp"],
        )

    @staticmethod
    def download(url: str) -> "Database":
        with urllib.request.urlopen(url) as response_stream:
            zip_stream = BytesIO(response_stream.read())
            with ZipFile(zip_stream) as zip_file:
                db_file = zip_file.namelist()[0]
                with zip_file.open(db_file) as db_stream:
                    return Database.from_dict(json.load(db_stream))


# TODO: create a SourceManager class that reads from ConfigSource and produces a list of files and folders
#       which can be used by the RemoteFileManager and the DatabaseBuilder to manage the files/folder and build the DB

@dataclass
class PathListFile:
    remote_name: str
    remote_url: str
    expected_size: int
    expected_hash: str
    local_name: str
    local_path: str


class PathList:
    files: dict[str, PathListFile]
    folders: set[str]

    def __init__(self):
        self.files = {}
        self.folders = set()


class SourceTransformer:
    output_path: str
    pathlist: PathList

    def __init__(self, output_path: str):
        self.output_path = output_path
        self.pathlist = PathList()

    @staticmethod
    def source_files(source: ConfigSource, db: Database) -> Iterable[Tuple[str, str]]:
        # if glob has no magic characters, we can improve performance by not using fnmatch
        for glob in source.entries:
            try:
                name = fnmatch.filter(db.files.keys(), glob)[0]
                yield name, name
            except IndexError:
                logger.error(f"No files matching '{glob}' found in {db.db_id}")

        for local_name, remote_glob in source.renames.items():
            try:
                remote_name = fnmatch.filter(db.files.keys(), remote_glob)[0]
                yield local_name, remote_name
            except IndexError:
                logger.error(f"No files matching '{remote_glob}' found in {db.db_id}")

    def add_source(self, source: ConfigSource) -> None:
        try:
            remote_db = Database.download(source.url)
        except IOError:
            logger.error(f"Unable to download source database from '{source.url}'")
            return

        for local_name, remote_name in self.source_files(source, remote_db):
            self.pathlist.folders.add(os.path.dirname(remote_name))
            self.pathlist.files[local_name] = PathListFile(
                remote_name=remote_name,
                remote_url=os.path.join(remote_db.base_files_url, urllib.parse.quote(remote_name)),
                expected_size=remote_db.files[remote_name].size,
                expected_hash=remote_db.files[remote_name].hash,
                local_name=local_name,
                local_path=os.path.join(self.output_path, local_name)
            )

    @property
    def db_files(self) -> dict[str, DatabaseFile]:
        return {
            f.local_name: DatabaseFile(
                hash=f.expected_hash,
                size=f.expected_size
            )
            for f in self.pathlist.files.values()
        }

    @property
    def db_folders(self) -> dict[str, DatabaseFolder]:
        return {f: DatabaseFolder() for f in self.pathlist.folders}


class FileManager:
    config: Config
    pathlist: PathList

    def __init__(self, config: Config, pathlist: PathList):
        self.config = config
        self.pathlist = pathlist

    @staticmethod
    def is_downloading_required(file: PathListFile) -> bool:
        try:
            with open(file.local_path, "rb") as file_stream:
                file_content = file_stream.read()
                if len(file_content) != file.expected_size:
                    logger.info(f"File '{file.local_name}' has invalid size")
                    return True
                file_hash = hashlib.md5(file_content).hexdigest()
                if file_hash != file.expected_hash:
                    logger.info(f"File '{file.local_name}' has invalid hash")
                    return True
        except FileNotFoundError:
            logger.info(f"File '{file.local_name}' not found")
            return True
        return False

    def download_files(self) -> None:
        for entry in self.pathlist.files.values():
            if self.is_downloading_required(entry):
                logger.debug(f"Downloading file '{entry.remote_name}'")
                with urllib.request.urlopen(entry.remote_url) as response_stream:
                    with open(entry.local_path, "wb") as output_stream:
                        output_stream.write(response_stream.read())

    def create_folders(self):
        for folder in self.pathlist.folders:
            local_path = os.path.join(self.config.output_path, folder)
            os.makedirs(local_path, exist_ok=True)


def main():
    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    if len(sys.argv) != 2:
        logger.critical(f"Usage: {os.path.basename(sys.argv[0])} <config_path>")
        sys.exit(1)

    try:
        config = Config.load(sys.argv[1])
    except IOError:
        logger.critical(f"Unable to load config file '{sys.argv[1]}'")
        sys.exit(1)

    transformer = SourceTransformer(config.output_path)
    for source in config.sources.values():
        transformer.add_source(source)

    file_manager = FileManager(config, transformer.pathlist)
    file_manager.create_folders()
    file_manager.download_files()

    new_db = Database(
        base_files_url=config.base_url,
        db_id=config.id,
        db_url=os.path.join(config.base_url, f"{config.id}.json.zip"),
        files=transformer.db_files,
        folders=transformer.db_folders,
        timestamp=round(time.time())
    )

    try:
        os.makedirs(config.output_path, exist_ok=True)
        new_db.write(os.path.join(config.output_path, f"{config.id}.json.zip"))
    except IOError:
        logger.critical(f"Unable to write proxied database to '{config.output_path}'")
        sys.exit(1)


if __name__ == "__main__":
    main()
