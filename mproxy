#!/usr/bin/env python
import hashlib
from enum import StrEnum
from io import BytesIO, TextIOWrapper
from dataclasses import dataclass, asdict
import itertools
import json
import logging
import os.path
import sys
import time
import urllib.request
import urllib.parse
from typing import Iterable, Tuple
from zipfile import ZipFile

logger = logging.getLogger(__name__)


class ConfigSourceType(StrEnum):
    REMOTE = "remote"
    LOCAL = "local"


@dataclass(frozen=True)
class ConfigSource:
    type: ConfigSourceType
    url: str
    entries: list[str]
    renames: dict[str, str]

    @property
    def files(self) -> Iterable[Tuple[str, str]]:
        # files in `entries` do not perform a rename, so proxy_name = src_name in that case
        return itertools.chain(((v, v) for v in self.entries), self.renames.items())

    @staticmethod
    def from_dict(d: dict) -> "ConfigSource":
        return ConfigSource(
            url=d["url"],
            type=ConfigSourceType(d["type"]),
            entries=d["entries"],
            renames=d["renames"],
        )


@dataclass(frozen=True)
class Config:
    id: str
    base_url: str
    output_path: str
    sources: dict[str, ConfigSource]

    @staticmethod
    def from_dict(d: dict) -> "Config":
        return Config(
            id=d["id"],
            base_url=d["base_url"],
            output_path=d["output_path"],
            sources={k: ConfigSource.from_dict(v) for k, v in d["sources"].items()},
        )

    @staticmethod
    def load(path: str) -> "Config":
        with open(path) as stream:
            return Config.from_dict(json.load(stream))


@dataclass(frozen=True)
class DatabaseFile:
    hash: str
    size: int

    @staticmethod
    def from_dict(d: dict) -> "DatabaseFile":
        return DatabaseFile(
            hash=d["hash"],
            size=d["size"]
        )


@dataclass(frozen=True)
class DatabaseFolder:
    pass


@dataclass(frozen=True)
class Database:
    base_files_url: str
    db_id: str
    db_url: str
    files: dict[str, DatabaseFile]
    folders: dict[str, DatabaseFolder]
    timestamp: int

    def write(self, path: str) -> None:
        with ZipFile(path, "w") as zip_file:
            db_file = f"{self.db_id}.json"
            with zip_file.open(db_file, "w") as db_stream:
                json.dump(asdict(self), TextIOWrapper(db_stream))

    @staticmethod
    def from_dict(d: dict) -> "Database":
        return Database(
            base_files_url=d["base_files_url"],
            db_id=d["db_id"],
            db_url=d["db_url"],
            files={k: DatabaseFile.from_dict(v) for k, v in d["files"].items()},
            folders={k: DatabaseFolder() for k in d["folders"].keys()},
            timestamp=d["timestamp"],
        )

    @staticmethod
    def download(url: str) -> "Database":
        with urllib.request.urlopen(url) as response_stream:
            zip_stream = BytesIO(response_stream.read())
            with ZipFile(zip_stream) as zip_file:
                db_file = zip_file.namelist()[0]
                with zip_file.open(db_file) as db_stream:
                    return Database.from_dict(json.load(db_stream))


# TODO: create a SourceManager class that reads from ConfigSource and produces a list of files and folders
#       which can be used by the RemoteFileManager and the DatabaseBuilder to manage the files/folder and build the DB

@dataclass
class RemoteFileManagerFile:
    remote_name: str
    remote_url: str
    expected_size: int
    expected_hash: str
    local_name: str
    local_path: str

    def is_downloading_required(self) -> bool:
        try:
            with open(self.local_path, "rb") as file_stream:
                file_content = file_stream.read()
                if len(file_content) != self.expected_size:
                    logger.info(f"File '{self.local_name}' has invalid size")
                    return True
                file_hash = hashlib.md5(file_content).hexdigest()
                if file_hash != self.expected_hash:
                    logger.info(f"File '{self.local_name}' has invalid hash")
                    return True
        except FileNotFoundError:
            logger.info(f"File '{self.local_name}' not found")
            return True
        return False

    def download(self) -> None:
        os.makedirs(os.path.dirname(self.local_path), exist_ok=True)
        with urllib.request.urlopen(self.remote_url) as response_stream:
            with open(self.local_path, "wb") as output_stream:
                output_stream.write(response_stream.read())


class RemoteFileManager:
    output_path: str
    files: list[RemoteFileManagerFile]
    folders: set[str]

    def __init__(self, output_path: str):
        self.output_path = output_path
        self.files = []
        self.folders = set()

    def add_source(self, source: ConfigSource) -> None:
        if source.type != ConfigSourceType.REMOTE:
            logger.error("Local sources not supported ")
            return

        try:
            remote_db = Database.download(source.url)
        except IOError:
            logger.error(f"Unable to download source database from '{source.url}'")
            return

        for local_name, remote_name in source.files:
            try:
                self.folders.add(os.path.dirname(remote_name))
                self.files.append(RemoteFileManagerFile(
                    remote_name=remote_name,
                    remote_url=os.path.join(remote_db.base_files_url, urllib.parse.quote(remote_name)),
                    expected_size=remote_db.files[remote_name].size,
                    expected_hash=remote_db.files[remote_name].hash,
                    local_name=local_name,
                    local_path=os.path.join(self.output_path, local_name)
                ))
            except KeyError:
                logger.error(f"Filename '{remote_name}' not found in {remote_db.db_id}")

    def download_files(self) -> None:
        for entry in self.files:
            if entry.is_downloading_required():
                logger.debug(f"Downloading file '{entry.remote_name}'")
                entry.download()

    def create_folders(self):
        for folder in self.folders:
            local_path = os.path.join(self.output_path, folder)
            os.makedirs(local_path, exist_ok=True)

    @property
    def db_files(self) -> dict[str, DatabaseFile]:
        return {
            f.local_name: DatabaseFile(
                hash=f.expected_hash,
                size=f.expected_size
            )
            for f in self.files
        }

    @property
    def db_folders(self) -> dict[str, DatabaseFolder]:
        return {f: DatabaseFolder() for f in self.folders}


def main():
    logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

    if len(sys.argv) != 2:
        logger.critical(f"Usage: {os.path.basename(sys.argv[0])} <config_path>")
        sys.exit(1)

    try:
        config = Config.load(sys.argv[1])
    except IOError:
        logger.critical(f"Unable to load config file '{sys.argv[1]}'")
        sys.exit(1)

    file_manager = RemoteFileManager(config.output_path)

    for source in config.sources.values():
        file_manager.add_source(source)

    file_manager.create_folders()
    file_manager.download_files()

    new_db = Database(
        base_files_url=config.base_url,
        db_id=config.id,
        db_url=os.path.join(config.base_url, f"{config.id}.json.zip"),
        files=file_manager.db_files,
        folders=file_manager.db_folders,
        timestamp=round(time.time())
    )

    try:
        os.makedirs(config.output_path, exist_ok=True)
        new_db.write(os.path.join(config.output_path, f"{config.id}.json.zip"))
    except IOError:
        logger.critical(f"Unable to write proxied database to '{config.output_path}'")
        sys.exit(1)


if __name__ == "__main__":
    main()
